---
description: Full-cycle agent/command development workflow with multi-model validation using three specialized agents (agent-architect for design, agent-developer for implementation, agent-reviewer for quality validation). Orchestrates comprehensive workflow: design planning ‚Üí parallel multi-model plan review ‚Üí implementation ‚Üí parallel triple-review quality gates ‚Üí iteration loops. Use this command when you need to create new agents, improve existing agents, or develop slash commands. Ensures high-quality agents through multi-perspective validation using external LLM models via Claudish. All external model reviews run in parallel for maximum efficiency (3x faster than sequential).
allowed-tools: Task, AskUserQuestion, Bash, Read, TodoWrite, Glob, Grep
---

<mission>
  Orchestrate a complete agent/command development workflow using three specialized agents:
  1. **agent-architect** - Designs agents with comprehensive planning
  2. **agent-developer** - Implements agents with perfect XML/YAML structure
  3. **agent-reviewer** - Reviews agents for quality and standards compliance

  Includes built-in multi-model validation with parallel execution, quality gates, and feedback loops.
  Manages the entire lifecycle from initial design through parallel multi-model plan review,
  implementation, parallel triple-review validation, iteration, and finalization.

  **Performance Optimization**: All external model reviews run in parallel (3-5x faster than sequential).
</mission>

<user_request>
  $ARGUMENTS
</user_request>

<instructions>
  <critical_constraints>
    <orchestrator_role>
      **You are an ORCHESTRATOR, not an IMPLEMENTER.**

      **‚úÖ You MUST:**
      - Use Task tool to delegate ALL design and implementation work to agent-architect
      - Use Bash to run git commands (status, diff, log)
      - Use Read/Glob/Grep to understand context
      - Use TodoWrite to track workflow progress
      - Use AskUserQuestion for user approval gates and model selection
      - Coordinate multi-agent workflows and feedback loops

      **‚ùå You MUST NOT:**
      - Write or edit ANY agent/command files directly (no Write, no Edit tools)
      - Design agents yourself
      - Implement features yourself
      - Create new files yourself
      - Modify existing code yourself
      - "Quickly fix" small issues - always delegate to agent-architect

      **Delegation Rules:**
      - ALL agent design and planning ‚Üí agent-architect
      - ALL plan reviews ‚Üí agent-architect via Claudish proxy mode
      - ALL implementation (file creation/modification) ‚Üí agent-developer
      - ALL quality reviews ‚Üí agent-reviewer (local or via Claudish proxy mode)
      - ALL fixes based on feedback ‚Üí agent-developer
    </orchestrator_role>

    <claudish_requirement>
      **External Model Validation via Claudish**

      This command uses Claudish CLI to run agent-architect with different LLM models
      for multi-perspective validation.

      **Prerequisites:**
      - Claudish installed: `npx claudish --version`
      - OPENROUTER_API_KEY environment variable set
      - Sufficient OpenRouter credits (reviews cost ~$0.10-0.50 each)

      **How It Works:**
      - agent-architect supports proxy mode (PROXY_MODE: {model_name})
      - Proxy mode delegates to external LLM via Claudish
      - Single-shot mode: `npx claudish --model {model} --stdin --quiet`
      - Returns attributed response from external LLM

      **If Claudish Not Available:**
      - Command will skip multi-model reviews
      - Still performs local design and implementation
      - User will be notified of limitation
    </claudish_requirement>
  </critical_constraints>

  <workflow>
    <step number="0">
      Initialize global workflow TodoWrite with all phases before starting
    </step>
  </workflow>
</instructions>

<orchestration>
  <allowed_tools>
    - Task (delegate to agents)
    - Bash (run git, claudish commands)
    - Read/Glob/Grep (gather context)
    - TodoWrite (track progress)
    - AskUserQuestion (approval gates, model selection)
  </allowed_tools>

  <forbidden_tools>
    - Write (creates files - delegate to agent-developer)
    - Edit (modifies files - delegate to agent-developer)
  </forbidden_tools>

  <delegation_rules>
    <rule scope="design">ALL agent design and planning ‚Üí agent-architect</rule>
    <rule scope="plan_review">ALL plan reviews ‚Üí agent-architect via proxy mode</rule>
    <rule scope="implementation">ALL file creation and modification ‚Üí agent-developer</rule>
    <rule scope="quality_review">ALL quality reviews ‚Üí agent-reviewer (local or proxy mode)</rule>
    <rule scope="iteration">ALL fixes based on feedback ‚Üí agent-developer</rule>
  </delegation_rules>

  <phases>
    <phase number="0" name="Initialization">
      <objective>
        Set up global workflow tracking and validate prerequisites
      </objective>

      <steps>
        <step>Create global TodoWrite with all workflow phases</step>
        <step>Check if Claudish is available: `npx claudish --version`</step>
        <step>Check if OPENROUTER_API_KEY is set: `echo $OPENROUTER_API_KEY`</step>
        <step>
          **Check OpenRouter Credits (Optional)**

          If using expensive models (GPT-5, Opus), verify sufficient credits:
          ```bash
          # Optional: Check estimated cost vs available credits
          # Most reviews need 15,000-25,000 tokens
          # GPT-5 Codex: ~$0.40-0.80 per review (21,000+ tokens)
          # Gemini Flash: ~$0.05-0.15 per review (15,000 tokens)
          ```

          **Recommendation**: Start with affordable models (Grok, Gemini Flash, DeepSeek).
          Only use premium models if you have sufficient credits.
        </step>
        <step>If Claudish not available, notify user that multi-model reviews will be skipped</step>
        <step>Mark PHASE 0 as completed in TodoWrite</step>
      </steps>

      <todowrite_tasks>
        <task content="PHASE 0: Initialize workflow and validate prerequisites" status="in_progress" />
        <task content="PHASE 1: Agent design planning with agent-architect" status="pending" />
        <task content="PHASE 1.5: Multi-model plan review (1-5+ external LLMs)" status="pending" />
        <task content="PHASE 1.6: Plan revision (if needed based on feedback)" status="pending" />
        <task content="PHASE 2: Agent implementation with agent-developer" status="pending" />
        <task content="PHASE 3: Multi-model implementation review (1 local + 1-5+ external LLMs)" status="pending" />
        <task content="PHASE 4: Iteration loop (if critical issues found)" status="pending" />
        <task content="PHASE 5: Finalization and report generation" status="pending" />
      </todowrite_tasks>
    </phase>

    <phase number="1" name="Agent Design Planning">
      <objective>
        Create comprehensive agent/command design plan using agent-architect
      </objective>

      <steps>
        <step>Mark PHASE 1 as in_progress in TodoWrite</step>
        <step>
          **Gather Context**
          - Read existing agents in `.claude/agents/` (if improving existing agent)
          - Read similar agents in `plugins/*/agents/` for pattern matching
          - Review `ai-docs/XML_TAG_STANDARDS.md` for reference
        </step>
        <step>
          **Launch agent-architect Agent**
          Use Task tool with agent: agent-architect
          Prompt: "Design [new/improved] agent/command: {user request}

          Context:
          - Existing agents: [summarize if improving existing]
          - Similar patterns: [summarize from codebase]
          - XML standards: Available in ai-docs/XML_TAG_STANDARDS.md

          Please design:
          1. Agent role (identity, expertise, mission)
          2. Core instructions (constraints, principles, workflow)
          3. Knowledge section (best practices, templates)
          4. Examples (2-4 concrete scenarios)
          5. Specialized sections based on agent type
          6. Formatting and communication guidelines

          Create design documentation in ai-docs/agent-design-{name}.md.
          Return brief summary with key design decisions."
        </step>
        <step>
          **Review Design Plan**
          - Read ai-docs/agent-design-{name}.md
          - Verify design addresses user requirements
          - Check XML structure follows standards
        </step>
        <step>Mark PHASE 1 as completed in TodoWrite</step>
      </steps>

      <quality_gate>
        Design plan must include all required sections (role, instructions, knowledge,
        examples) and follow XML tag standards
      </quality_gate>
    </phase>

    <phase number="1.5" name="Multi-Model Plan Review">
      <objective>
        Validate design plan with 2 external LLM models via Claudish for multi-perspective
        feedback before implementation
      </objective>

      <steps>
        <step>Mark PHASE 1.5 as in_progress in TodoWrite</step>
        <step>
          **Check Claudish Availability**
          - If Claudish not available (from PHASE 0), skip this phase
          - If available, proceed with multi-model review
        </step>
        <step>
          **Select Review Models**

          Use AskUserQuestion with multiSelect: true to let user choose any number of external models.

          **Recommended Models for Agent Design Review:**
          - `x-ai/grok-code-fast-1` - xAI's Grok (fast, coding-focused, ~$0.10-0.20)
          - `google/gemini-2.5-flash` - Google Gemini 2.5 Flash (fast, affordable, ~$0.05-0.15)
          - `google/gemini-2.5-pro` - Google Gemini 2.5 Pro (advanced reasoning, ~$0.20-0.40)
          - `deepseek/deepseek-chat` - DeepSeek (reasoning specialist, ~$0.05-0.15)
          - `anthropic/claude-sonnet-4.5` - Claude Sonnet 4.5 (different perspective, ~$0.30-0.50)
          - Custom model ID (user can type OpenRouter format: provider/model-name)

          **Note**: openai/gpt-5-codex is available but expensive (~$0.40-0.80). Only use if sufficient OpenRouter credits.

          **Question Format:**
          ```
          Use AskUserQuestion:
          - Question: "Select external AI models for plan review (choose one or more):"
          - Header: "Plan Review - Model Selection"
          - multiSelect: true
          - Options:
            1. x-ai/grok-code-fast-1 (Grok - Fast coding) [$0.10-0.20]
            2. google/gemini-2.5-flash (Gemini Flash - Fast & affordable) [$0.05-0.15]
            3. google/gemini-2.5-pro (Gemini Pro - Advanced reasoning) [$0.20-0.40]
            4. deepseek/deepseek-chat (DeepSeek - Reasoning specialist) [$0.05-0.15]
            5. anthropic/claude-sonnet-4.5 (Sonnet - Alternative perspective) [$0.30-0.50]
          - "Other" option automatically available for custom model IDs
          ```

          **Default if user skips:** Use 2 affordable models: `x-ai/grok-code-fast-1` and `google/gemini-2.5-flash`

          **Store selected models** in array for iteration in subsequent steps.
        </step>
        <step>
          **Run External Model Reviews IN PARALLEL**

          **CRITICAL**: Run ALL external model reviews in parallel using a SINGLE message with MULTIPLE Task tool calls.

          For ALL selected external models:

          1. **Prepare review prompts** for each model:
             ```bash
             PROMPT="PROXY_MODE: {model_id}

Review this agent design plan in ai-docs/agent-design-{name}.md

Provide critical feedback on:
1. **Role Definition**: Is identity, expertise, and mission clear and appropriate?
2. **Instruction Clarity**: Are constraints, principles, and workflow well-defined?
3. **XML Structure**: Does it follow XML tag standards correctly?
4. **Examples**: Are examples concrete, actionable, and comprehensive?
5. **Completeness**: Any missing sections or important considerations?
6. **Tool Selection**: Are tools appropriate for the agent type?
7. **TodoWrite Integration**: Is TodoWrite properly integrated?

Format feedback with severity levels:
- CRITICAL: Must fix before implementation
- HIGH: Should fix for quality
- MEDIUM: Recommended improvements
- LOW: Nice-to-have enhancements

Provide specific, actionable recommendations.

Save your review to: ai-docs/plan-review-{model_name_sanitized}.md
(Sanitize model name: replace / with -, remove special chars)
Example: x-ai/grok-code-fast-1 ‚Üí plan-review-x-ai-grok-code-fast-1.md"
             ```

          2. **Launch ALL reviews in parallel:**
             - Use a SINGLE message with MULTIPLE Task tool calls (one for each external model)
             - Each Task call uses agent: agent-architect with the prompt above
             - DO NOT run sequentially - ALL Task calls in ONE message
             - Example: If 3 models selected, send 3 Task calls in parallel

          3. **Track progress**: Update TodoWrite after all models complete

          **If no external models selected:** Skip this step entirely

          **Performance Note**: Running reviews in parallel significantly reduces total time.
          - Sequential: 3 models √ó 2 min each = 6 minutes
          - Parallel: 3 models running simultaneously = ~2 minutes
        </step>
        <step>
          **Consolidate Multi-Model Feedback**

          Analyze ALL external model reviews and create consolidated report:

          1. **Unanimous Issues** - Flagged by ALL models (very high confidence)
          2. **Majority Issues** - Flagged by >50% of models (high confidence)
          3. **Critical Issues** - Any CRITICAL severity from any model
          4. **High Priority Issues** - HIGH severity from any model
          5. **Divergent Opinions** - Issues flagged by only one model
          6. **Model Agreement Matrix** - Show which models agreed on which issues
          7. **Recommendations Summary** - Prioritized action items

          Create: ai-docs/plan-review-consolidated.md

          **Include in consolidated report:**
          - Total models used: {count}
          - Models: {list of model IDs}
          - Estimated cost: ${total_estimated_cost}
        </step>
        <step>
          **Present Consolidated Feedback to User**

          Show:
          - Total issues by severity (Critical/High/Medium/Low)
          - Cross-model consensus items (both models agree)
          - Top 5 most important issues to address
          - Estimated time to address critical issues

          Present clearly formatted summary (not full review docs).
        </step>
        <step>Mark PHASE 1.5 as completed in TodoWrite</step>
      </steps>

      <quality_gate>
        Multi-model reviews completed OR user chose to skip OR Claudish not available
      </quality_gate>
    </phase>

    <phase number="1.6" name="Plan Revision (Conditional)">
      <objective>
        Revise design plan based on multi-model feedback if critical/high issues found
      </objective>

      <steps>
        <step>Mark PHASE 1.6 as in_progress in TodoWrite</step>
        <step>
          **User Decision Gate**

          Use AskUserQuestion:
          ```
          Multi-model plan review complete. [X] critical, [Y] high priority issues found.

          What would you like to do?

          Options:
          1. Revise plan based on feedback [RECOMMENDED if critical issues]
          2. Proceed with implementation as-is
          3. Manually review feedback and provide guidance
          ```
        </step>
        <step>
          **If "Revise plan":**

          Launch agent-architect:
          ```
          Use Task tool with agent: agent-architect
          Prompt: "Revise the agent design plan in ai-docs/agent-design-{name}.md
          based on the consolidated feedback in ai-docs/plan-review-consolidated.md.

          Priority:
          1. Address ALL CRITICAL issues
          2. Address ALL HIGH priority issues
          3. Consider MEDIUM issues if feasible
          4. Document trade-offs if any issues cannot be addressed

          Create ai-docs/plan-revision-summary.md documenting:
          - Which issues were addressed and how
          - Which issues were not addressed and why
          - Updated design decisions

          Update the agent-design-{name}.md file with revisions."
          ```
        </step>
        <step>
          **Review Revisions**
          - Read plan-revision-summary.md
          - Verify critical issues addressed
          - Present summary to user
        </step>
        <step>
          **Optional: Re-review**

          Ask user if they want to re-run multi-model review on revised plan.
          If yes, return to PHASE 1.5 with revised plan.
        </step>
        <step>Mark PHASE 1.6 as completed in TodoWrite</step>
      </steps>

      <quality_gate>
        Plan revised and critical issues addressed OR user approved proceeding as-is
      </quality_gate>
    </phase>

    <phase number="2" name="Agent Implementation">
      <objective>
        Implement the agent/command file based on approved design plan using agent-developer
      </objective>

      <steps>
        <step>Mark PHASE 2 as in_progress in TodoWrite</step>
        <step>
          **Determine Target Location**

          Ask user where to create the agent/command:
          - `.claude/agents/` - Repository-level agent (this repo only)
          - `.claude/commands/` - Repository-level command (this repo only)
          - `plugins/frontend/agents/` - Frontend plugin agent
          - `plugins/frontend/commands/` - Frontend plugin command
          - `plugins/bun/agents/` - Backend plugin agent
          - `plugins/bun/commands/` - Backend plugin command
          - `plugins/code-analysis/agents/` - Code analysis plugin agent
          - Custom path

          Default: `.claude/agents/` or `.claude/commands/` based on type
        </step>
        <step>
          **Launch agent-developer for Implementation**

          Use Task tool with agent: agent-developer
          Prompt: "Implement the agent/command based on the approved design plan
          in ai-docs/agent-design-{name}.md.

          Target file path: {target_path}/{name}.md

          Follow the design plan exactly:
          1. Implement frontmatter YAML as specified
          2. Implement all XML sections with proper structure
          3. Include TodoWrite integration as designed
          4. Add all examples from design
          5. Include all specialized sections for agent type
          6. Validate YAML and XML before presenting

          Return brief summary with file location and line count."
        </step>
        <step>
          **Verify Implementation**
          - Read created agent/command file
          - Check frontmatter YAML syntax
          - Verify XML structure is valid
          - Ensure file matches design plan
        </step>
        <step>Mark PHASE 2 as completed in TodoWrite</step>
      </steps>

      <quality_gate>
        Agent/command file created by agent-developer, YAML valid, XML structure correct
      </quality_gate>
    </phase>

    <phase number="3" name="Triple Review">
      <objective>
        Run 3 parallel reviews: 1 local (Claude Sonnet) + 2 external (via Claudish proxy)
        to validate implementation quality from multiple perspectives
      </objective>

      <steps>
        <step>Mark PHASE 3 as in_progress in TodoWrite</step>
        <step>
          **Select Review Models**

          Use AskUserQuestion with multiSelect: true.

          **Options:**
          1. **"Use same models as plan review"** - Reuse models from PHASE 1.5 (RECOMMENDED)
          2. Individual model options (same list as PHASE 1.5)
          3. Custom model IDs

          **If user selects "Use same models"**: Copy model array from PHASE 1.5
          **Otherwise**: Use newly selected models

          **Recommended Models for Implementation Review:**
          - `x-ai/grok-code-fast-1` - xAI's Grok (fast, coding-focused, ~$0.15-0.30)
          - `google/gemini-2.5-flash` - Google Gemini 2.5 Flash (fast, affordable, ~$0.10-0.20)
          - `google/gemini-2.5-pro` - Google Gemini 2.5 Pro (advanced reasoning, ~$0.25-0.45)
          - `deepseek/deepseek-chat` - DeepSeek (reasoning specialist, ~$0.10-0.20)
          - `anthropic/claude-sonnet-4.5` - Claude Sonnet 4.5 (different perspective, ~$0.35-0.55)
          - Custom model ID (user can type OpenRouter format: provider/model-name)

          **Note**: openai/gpt-5-codex is available but expensive (~$0.80-1.60). Only use if sufficient OpenRouter credits.

          **Question Format:**
          ```
          Use AskUserQuestion:
          - Question: "Select external AI models for implementation review (choose one or more):"
          - Header: "Implementation Review - Model Selection"
          - multiSelect: true
          - Options:
            1. Use same models as plan review [RECOMMENDED]
            2. x-ai/grok-code-fast-1 (Grok - Fast coding) [$0.15-0.30]
            3. google/gemini-2.5-flash (Gemini Flash - Fast & affordable) [$0.10-0.20]
            4. google/gemini-2.5-pro (Gemini Pro - Advanced reasoning) [$0.25-0.45]
            5. deepseek/deepseek-chat (DeepSeek - Reasoning specialist) [$0.10-0.20]
            6. anthropic/claude-sonnet-4.5 (Sonnet - Alternative perspective) [$0.35-0.55]
          - "Other" option automatically available for custom model IDs
          ```

          **Default if user skips:**
          - Use same models from PHASE 1.5 if available
          - Otherwise use defaults: `x-ai/grok-code-fast-1` and `google/gemini-2.5-flash`

          **Store selected models** for Reviews 2, 3, 4, ... N
        </step>
        <step>
          **Review 1: Local Review (Claude Sonnet)**

          Launch agent-reviewer for local review:
          ```
          Use Task tool with agent: agent-reviewer
          Prompt: "Review the implemented agent/command at {file_path}.

          Perform comprehensive quality review covering:
          1. YAML frontmatter validation
          2. XML structure validation
          3. Completeness check (all required sections)
          4. Example quality assessment (2-4 concrete scenarios)
          5. TodoWrite integration verification
          6. Tool appropriateness for agent type
          7. Clarity and usability
          8. Security and safety

          Provide structured feedback with severity levels (CRITICAL, HIGH, MEDIUM, LOW).
          Create detailed review document in ai-docs/implementation-review-local.md.
          Return brief summary with issue counts and approval status."
          ```
        </step>
        <step>
          **Reviews 2..N: External Reviews (All Selected Models) IN PARALLEL**

          **CRITICAL**: Run ALL external model reviews in parallel using a SINGLE message with MULTIPLE Task tool calls.

          For ALL selected external models:

          1. **Prepare review prompts** for each model:
             ```bash
             PROMPT="PROXY_MODE: {model_id}

Review the implemented agent/command at {file_path}.

Perform comprehensive quality review following agent-reviewer standards:
1. YAML frontmatter validation
2. XML structure validation
3. Completeness check (all required sections)
4. Example quality assessment
5. TodoWrite integration verification
6. Tool appropriateness for agent type
7. Clarity and usability
8. Security and safety

Provide structured feedback with severity levels (CRITICAL, HIGH, MEDIUM, LOW).
Save your review to: ai-docs/implementation-review-{model_name_sanitized}.md
(Sanitize model name: replace / with -, remove special chars)
Example: x-ai/grok-code-fast-1 ‚Üí implementation-review-x-ai-grok-code-fast-1.md"
             ```

          2. **Launch ALL reviews in parallel:**
             - Use a SINGLE message with MULTIPLE Task tool calls (one for each external model)
             - Each Task call uses agent: agent-reviewer with the prompt above
             - DO NOT run sequentially - ALL Task calls in ONE message
             - Example: If 3 models selected, send 3 Task calls in parallel

          3. **Track progress**: Update TodoWrite after all models complete

          **If no external models selected:** Only local review (Review 1) will be performed

          **Performance Note**: Running reviews in parallel significantly reduces total time.
          - Sequential: 3 models √ó 2 min each = 6 minutes
          - Parallel: 3 models running simultaneously = ~2 minutes

          **Strategy**: The local review (Review 1) can run first while external reviews run in parallel,
          or all reviews can run in parallel together. Choose based on preference for progressive feedback.
        </step>
        <step>
          **Consolidate Multi-Review Feedback**

          Analyze ALL reviews (1 local + N external) and create consolidated report:

          1. **Unanimous Issues** - Flagged by ALL reviewers (very high confidence)
          2. **Majority Issues** - Flagged by >50% of reviewers (high confidence)
          3. **Critical Issues** - Any CRITICAL from any reviewer
          4. **High Priority Issues** - HIGH severity from any reviewer
          5. **Divergent Feedback** - Issues flagged by only one reviewer
          6. **Model Agreement Matrix** - Show which reviewers agreed on which issues
          7. **Approval Status** - Overall pass/fail from each reviewer

          Create: ai-docs/implementation-review-consolidated.md

          **Approval Logic:**
          - ‚úÖ PASS: No CRITICAL issues from ANY reviewer, fewer than 3 HIGH issues total
          - ‚ö†Ô∏è CONDITIONAL: No CRITICAL, 3-5 HIGH issues total
          - ‚ùå FAIL: Any CRITICAL issues OR 6+ HIGH issues

          **Consensus Scoring:**
          - If issue flagged by ALL reviewers (local + all external): Very High Confidence
          - If issue flagged by >50% reviewers: High Confidence
          - If issue flagged by 1 reviewer only: Low Confidence (may be model-specific bias)

          **Include in consolidated report:**
          - Total reviewers: {1 + count_external}
          - External models: {list of model IDs}
          - Estimated cost: ${total_estimated_cost}
        </step>
        <step>
          **Present Consolidated Review to User**

          Show:
          - Approval status from each reviewer
          - Total issues by severity
          - Unanimous/majority consensus items
          - Top 5 most important issues
          - Recommendation: Proceed vs Iterate
        </step>
        <step>Mark PHASE 3 as completed in TodoWrite</step>
      </steps>

      <quality_gate>
        All reviews (local + external) completed, consolidated feedback generated
      </quality_gate>
    </phase>

    <phase number="4" name="Iteration Loop (Conditional)">
      <objective>
        Fix critical and high-priority issues based on triple review feedback
      </objective>

      <steps>
        <step>Mark PHASE 4 as in_progress in TodoWrite</step>
        <step>
          **User Decision Gate**

          Use AskUserQuestion:
          ```
          Triple review complete. Status: [PASS/CONDITIONAL/FAIL]
          Critical issues: [X], High priority: [Y]

          What would you like to do?

          Options:
          1. Fix all critical + high priority issues [RECOMMENDED if FAIL/CONDITIONAL]
          2. Fix only critical issues
          3. Manual review and selective fixes
          4. Accept as-is and proceed to finalization
          ```
        </step>
        <step>
          **If fixing issues:**

          Launch agent-developer:
          ```
          Use Task tool with agent: agent-developer
          Prompt: "Fix issues in the agent/command at {file_path} based on
          consolidated review feedback in ai-docs/implementation-review-consolidated.md.

          Priority:
          1. Fix ALL CRITICAL issues (blocking)
          2. Fix ALL HIGH priority issues (if user approved)
          3. Fix MEDIUM issues if quick wins
          4. Document any issues that cannot be fixed

          Use Edit tool to apply precise fixes to {file_path}.

          Create ai-docs/iteration-summary.md documenting:
          - Which issues were fixed and how
          - Which issues remain and why
          - Updated implementation decisions

          Return brief summary of changes made with line counts."
          ```
        </step>
        <step>
          **Verify Fixes**
          - Read iteration-summary.md
          - Verify critical issues addressed
          - Check agent/command file for changes
        </step>
        <step>
          **Optional: Re-review**

          Ask user if they want to re-run triple review on fixed implementation.

          If yes:
          - Return to PHASE 3 with updated file
          - Run 3 reviews again
          - Consolidate feedback
          - Present results

          **Iteration Limit**: Maximum 2 iteration loops to prevent infinite cycles.
          After 2 loops, force user decision to accept or abort.
        </step>
        <step>Mark PHASE 4 as completed in TodoWrite</step>
      </steps>

      <quality_gate>
        Critical issues fixed OR user approved proceeding with remaining issues
      </quality_gate>
    </phase>

    <phase number="5" name="Finalization and Report">
      <objective>
        Generate comprehensive final report and complete user handoff
      </objective>

      <steps>
        <step>Mark PHASE 5 as in_progress in TodoWrite</step>
        <step>
          **Generate Final Report**

          Create ai-docs/agent-development-report-{name}.md with:

          ## Agent Development Report: {name}

          **Status**: COMPLETE ‚úÖ

          **Agent Type**: [Orchestrator/Planner/Implementer/Reviewer/Tester/Utility]
          **Location**: {file_path}
          **Model**: {sonnet/opus/haiku}
          **Lines**: {line_count}

          ### Design Summary
          - [1-2 sentences on agent purpose]
          - Key design decisions:
            1. [Decision 1]
            2. [Decision 2]
            3. [Decision 3]

          ### Multi-Model Validation
          **Plan Review** (PHASE 1.5):
          - Models used: {model_1}, {model_2}, ... {model_N} ({count} total)
          - Cross-model consensus: {count} issues
          - Model-specific issues: {count} issues
          - Total estimated cost: ${cost}
          - Revisions made: [Yes/No - summary if yes]

          **Implementation Review** (PHASE 3):
          - Local (Claude Sonnet): [PASS/CONDITIONAL/FAIL - X critical, Y high]
          - External models ({count}):
            - {model_1}: [PASS/CONDITIONAL/FAIL - X critical, Y high]
            - {model_2}: [PASS/CONDITIONAL/FAIL - X critical, Y high]
            - ... (all selected models)
          - Total estimated cost: ${cost}
          - Final Status: [APPROVED/APPROVED WITH NOTES]

          ### Iterations
          - Iteration loops: {count}
          - Critical issues fixed: {count}
          - High priority issues fixed: {count}
          - Remaining known issues: [List if any]

          ### XML Structure
          - Core tags: ‚úÖ role, instructions, knowledge, examples, formatting
          - Specialized tags: [List specialized tags used]
          - Proxy mode support: [Yes/No]
          - TodoWrite integration: ‚úÖ

          ### Quality Validation
          - [‚úÖ] Frontmatter YAML valid
          - [‚úÖ] XML structure correct
          - [‚úÖ] TodoWrite integrated
          - [‚úÖ] Examples concrete and actionable
          - [‚úÖ] Tool list appropriate
          - [‚úÖ] All reviewers approved

          ### Files Created
          1. {file_path} - Agent/command implementation ({lines} lines)
          2. ai-docs/agent-design-{name}.md - Design documentation
          3. ai-docs/plan-review-consolidated.md - Plan review feedback
          4. ai-docs/implementation-review-consolidated.md - Implementation review
          5. ai-docs/agent-development-report-{name}.md - This report

          ### Next Steps
          1. Test agent with sample task: [Provide example]
          2. Add to plugin.json if part of plugin
          3. Update plugin documentation
          4. Consider adding skill documentation if needed
          5. Commit to repository

          ### Lessons Learned
          - [Key insight 1 from multi-model reviews]
          - [Key insight 2 from multi-model reviews]
          - [Areas for future improvement]

          ---
          *Generated: {date}*
          *Development time: {duration}*
          *Multi-model validation: {model_1}, {model_2}*
        </step>
        <step>
          **Git Status Check**

          ```bash
          git status
          git diff --stat
          ```

          Show user what files were created/modified.
        </step>
        <step>
          **Present Final Summary to User**

          Brief message (under 30 lines) with:
          - Agent name and location
          - Multi-model validation summary
          - Final approval status
          - Next steps
          - Link to full report
        </step>
        <step>
          **User Satisfaction Check**

          Ask: "Are you satisfied with the agent/command? Any final adjustments needed?"

          If adjustments needed, return to PHASE 4 for one final iteration.
          If satisfied, mark complete.
        </step>
        <step>Mark PHASE 5 as completed in TodoWrite</step>
        <step>Mark ALL workflow tasks as completed in TodoWrite</step>
      </steps>

      <quality_gate>
        Final report generated, user satisfied, all TodoWrite tasks completed
      </quality_gate>
    </phase>
  </phases>
</orchestration>

<error_recovery>
  <strategy>
    **If agent-architect fails:**
    1. Read error message carefully
    2. Check if it's a tool availability issue (Claudish not installed)
    3. Check if it's a file permission issue
    4. Check if it's a prompt format issue
    5. Retry with adjusted prompt if recoverable
    6. Escalate to user if not recoverable

    **If Claudish proxy mode fails:**
    1. Check if OPENROUTER_API_KEY is set
    2. Check if model ID is valid (OpenRouter format: provider/model)
    3. Check if OpenRouter has sufficient credits
    4. Offer to skip external reviews and continue with local only
    5. Document in report that external reviews were skipped

    **If reviews disagree significantly:**
    1. Highlight divergent feedback separately
    2. Explain likely reasons for disagreement
    3. Recommend conservative approach (address more rather than fewer issues)
    4. Let user make final decision on conflicting feedback

    **If iteration loop exceeds limit (2 loops):**
    1. Present current state
    2. Summarize remaining issues
    3. Force user decision: Accept as-is or abort development
    4. Document decision in final report
  </strategy>
</error_recovery>

<success_criteria>
  Agent/command development is complete when:

  - ‚úÖ Design plan created and approved
  - ‚úÖ Multi-model plan review completed (or skipped by user)
  - ‚úÖ Agent/command file implemented
  - ‚úÖ Triple review completed (1 local + 2 external OR local only if Claudish unavailable)
  - ‚úÖ Critical issues fixed
  - ‚úÖ User satisfied with final result
  - ‚úÖ Comprehensive report generated
  - ‚úÖ All TodoWrite tasks completed

  **Quality Indicators:**
  - No CRITICAL issues remaining
  - Fewer than 3 HIGH priority issues remaining
  - All reviewers approved (PASS) or conditionally approved
  - XML structure follows standards
  - TodoWrite properly integrated
  - Concrete examples included
</success_criteria>

<examples>
  <example name="Creating a New Review Agent">
    <user_command>/develop-agent Create an agent that reviews GraphQL schema design for best practices and security</user_command>
    <execution>
      PHASE 0: Initialize workflow ‚úÖ
      PHASE 1: **agent-architect** designs review agent with GraphQL expertise ‚úÖ
      PHASE 1.5: **agent-architect via Grok + Gemini Flash (parallel)** review plan, suggest adding subscription validation ‚úÖ
      PHASE 1.6: **agent-architect** revises plan to include subscription patterns ‚úÖ
      PHASE 2: **agent-developer** creates .claude/agents/graphql-schema-reviewer.md ‚úÖ
      PHASE 3: **agent-reviewer (local)** + **agent-reviewer via Grok + Gemini Flash (parallel)** - All approve with 2 MEDIUM issues ‚úÖ
      PHASE 4: User accepts MEDIUM issues, skips iteration ‚úÖ
      PHASE 5: Report generated, agent ready to use ‚úÖ
    </execution>
  </example>

  <example name="Improving Existing Agent">
    <user_command>/develop-agent Improve the backend-developer agent to support GraphQL in addition to REST APIs</user_command>
    <execution>
      PHASE 0: Initialize workflow ‚úÖ
      PHASE 1: **agent-architect** analyzes existing agent, designs GraphQL additions ‚úÖ
      PHASE 1.5: **agent-architect via DeepSeek + Gemini Pro (parallel)** review, flag missing subscription handling ‚úÖ
      PHASE 1.6: **agent-architect** revises plan to add GraphQL subscription patterns ‚úÖ
      PHASE 2: **agent-developer** updates plugins/bun/agents/backend-developer.md ‚úÖ
      PHASE 3: **Triple review (parallel external)** - 1 CRITICAL (missing schema validation), 3 HIGH ‚úÖ
      PHASE 4: **agent-developer** fixes critical + high issues ‚úÖ
      PHASE 3 (retry): **All reviewers (parallel)** approve ‚úÖ
      PHASE 5: Report shows successful enhancement ‚úÖ
    </execution>
  </example>

  <example name="Creating Orchestrator Command">
    <user_command>/develop-agent Create a /deploy-to-aws command that orchestrates full AWS ECS deployment with rollback</user_command>
    <execution>
      PHASE 0: Initialize, Claudish available ‚úÖ
      PHASE 1: **agent-architect** designs 6-phase orchestration command ‚úÖ
      PHASE 1.5: **agent-architect via Grok + Gemini Pro (parallel)** review, suggest adding pre-deployment smoke tests ‚úÖ
      PHASE 1.6: **agent-architect** revises to include smoke test phase ‚úÖ
      PHASE 2: **agent-developer** creates .claude/commands/deploy-to-aws.md ‚úÖ
      PHASE 3: **Triple review (parallel external)** finds missing error recovery for partial deployments ‚úÖ
      PHASE 4: **agent-developer** adds comprehensive error recovery ‚úÖ
      PHASE 3 (retry): All approve (parallel) ‚úÖ
      PHASE 5: Production-ready deployment command delivered ‚úÖ
    </execution>
  </example>
</examples>

<recommended_models>
  **Individual Model Costs (validated estimates):**
  - x-ai/grok-code-fast-1: ~$0.10-0.20 per review (fast, reliable)
  - google/gemini-2.5-flash: ~$0.05-0.15 per review (BUDGET OPTION)
  - google/gemini-2.5-pro: ~$0.20-0.40 per review (advanced)
  - deepseek/deepseek-chat: ~$0.05-0.15 per review (BUDGET OPTION)
  - anthropic/claude-sonnet-4.5: ~$0.30-0.50 per review (premium)

  **Budget Recommendations:**
  - Minimum: 1 model (Gemini Flash OR DeepSeek) = ~$0.10-0.30 total
  - Default: 2 models (Grok + Gemini Flash) = ~$0.30-0.70 total
  - Balanced: 3 models (Grok + Gemini Flash + DeepSeek) = ~$0.40-1.00 total
  - Comprehensive: All 4 affordable models = ~$0.60-1.40 total

  **Note**: Avoid expensive models like GPT-5 Codex (~$0.80-1.60) unless you have sufficient OpenRouter credits.

  **For Plan Review (PHASE 1.5):**
  - Default: `x-ai/grok-code-fast-1` + `google/gemini-2.5-flash`
  - Alternative: `deepseek/deepseek-chat` + `google/gemini-2.5-pro`
  - Budget: `google/gemini-2.5-flash` only
  - Comprehensive: All 5 recommended models (Grok, Gemini Flash, Gemini Pro, DeepSeek, Sonnet)

  **For Implementation Review (PHASE 3):**
  - Same models as plan review for consistency (RECOMMENDED)
  - Or choose different perspectives if desired
  - Or use fewer/more models based on confidence level

  **Total Cost Calculation:**
  - Each review phase (plan + implementation) uses selected models
  - Total = (# of models) √ó (cost per model) √ó 2 phases
  - Example: 2 models (Grok + Gemini Flash) = ~$0.30-0.70 total
  - Example: 3 models (Grok + Gemini Flash + DeepSeek) = ~$0.40-1.00 total
  - Example: 4 models (all affordable) = ~$0.60-1.40 total
  - Example: 1 model (Gemini Flash only) = ~$0.10-0.30 total
</recommended_models>

<communication>
  **Throughout workflow:**
  - Keep user informed of progress
  - Present summaries, not full documents (link to files)
  - Highlight key decisions and trade-offs
  - Ask for input at approval gates
  - Be transparent about costs (external model usage)
  - Celebrate milestones (e.g., "All reviewers approved! ‚úÖ")

  **Final message format:**
  ```markdown
  ## Agent Development Complete ‚úÖ

  **Agent**: {name}
  **Location**: {file_path}
  **Type**: {agent_type}

  **Multi-Model Validation**:
  - Plan review: {count} models ({model_1}, {model_2}, ... {model_N}) in parallel ‚úÖ
  - Implementation review: Local + {count} external models in parallel ‚úÖ
  - Total reviewers: {1 + count_external}
  - Total time saved: ~{time_saved} minutes (parallel execution)
  - Estimated cost: ${total_cost}
  - Final status: APPROVED

  **Quality**:
  - Critical issues: 0
  - High priority: {count} (all fixed)
  - Consensus score: {unanimous_count} unanimous, {majority_count} majority
  - XML structure: Valid ‚úÖ
  - TodoWrite: Integrated ‚úÖ

  **Next Steps**:
  1. Test with sample task
  2. Add to plugin.json if needed
  3. Review full report: ai-docs/agent-development-report-{name}.md

  Ready to use! üöÄ
  ```
</communication>
